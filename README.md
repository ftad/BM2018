# Курс Байесовские методы анализа данных, ФТиАД 2018

### Где и когда
Занятия проходят по понедельникам, 18:00 — 21:00, ШАД, Оксфорд.

### Ссылки
Канал в telegram для объявлений: https://t.me/joinchat/AAAAAE-WyZ3yUXyJWQORLg

[Anytask курса](https://anytask.org/course/393) (если нет логина, зарегистрироваться. инвайт 6i8JY8t)

### Правила выставления оценок
В курсе предусмотрено несколько форм контроля знания:
* Домашние работы (на Python и теоретические)
* Экзамен

Итоговая оценка вычисляется на основе оценки за работу в семестре и оценки за экзамен:

O<sub>итоговая</sub> = 0.7 * О<sub>накопленная</sub> + 0.3 * О<sub>экз</sub>

O<sub>накопленная</sub> = О<sub>домашние задания</sub>

Оценка за домашнюю работу вычисляется как среднее по домашним заданиям.

Накопленная, экзаменационная и итоговая оценки округляются арифметически.

### Правила сдачи домашних заданий

Дедлайн по заданиям — начало следующего занятия (жесткий).

Два раза за курс дедлайн можно просрочить, в этом случае за каждый день просрочки будет начисляться штраф -1 балл.

При обнаружении плагиата оценки за домашнее задание обнуляются всем задействованным в списывании студентам.

### Материалы занятий
__Занятие 1. Байесовские рассуждения__
* [Видео лекции](https://www.youtube.com/playlist?list=PLEqoHzpnmTfCiJpMPccTWXD9DB4ERQkyw)
* [Конспект лекции](https://drive.google.com/file/d/13Q58mRGh5uN8xyhMiTfoOXOYvxUKbvRY/view)
* [Еще один конспект](http://www.machinelearning.ru/wiki/images/8/8c/Lecture7_2012.pdf)
* [Задачи семинара](http://www.machinelearning.ru/wiki/images/1/18/S01_bayesian_reasoning_2016.pdf)

__Занятие 2. Сопряженные распределения и принцип наибольшей обоснованности__
* [Конспект-презентация](http://www.machinelearning.ru/wiki/images/b/bd/BMMO11_5.pdf)
* [Презентация с распределениями](https://github.com/ftad/BM2018/blob/master/materials/distributions.pdf)

__Занятие 3. Экспоненциальный класс распределений и матричное дифференцирование__
* [Конспект об экспоненциальном классе](https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf) от Michael I. Jordan
* [Конспект о матричном дифференцировании и нормальном распределении](http://www.machinelearning.ru/wiki/images/6/6c/BMMO11_8.pdf)
* [О матричном дифференцировании](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) не очень доступно но полно
* [О матричном дифференцировании](http://www.machinelearning.ru/wiki/images/a/ab/MOMO18_Seminar1.pdf) не очень полно но доступно и с кучей примеров

__Занятие 4. Метод релевантных векторов__
* [Конспект-презентация](http://www.machinelearning.ru/wiki/images/d/d0/BMMO11_7.pdf)
* [Презентация с экспериментами](http://www.machinelearning.ru/wiki/images/8/8d/BMML15_S06_show.pdf)

### Задания
* [Домашнее задание 1](https://github.com/ftad/BM2018/blob/master/homeworks/homework1.pdf). Дедлайн: 18:00 17.09.18
* [Домашнее задание 2](https://github.com/ftad/BM2018/blob/master/homeworks/homework2.pdf). Дедлайн: 18:00 24.09.18. Сдавать в Anytask!
* [Домашнее задание 3](https://github.com/ftad/BM2018/blob/master/homeworks/homework3.pdf). Дедлайн: 18:00 01.10.18. Сдавать в Anytask!
* [Домашнее задание 4 (пр.)](https://github.com/ftad/BM2018/blob/master/homeworks/homework4.ipynb) Дедлайн: 18:00 08.10.18. Сдавать в Anytask!

### Полезные материалы
Книги:
* Barber D. [Bayesian Reasoning and Machine Learning.](http://www0.cs.ucl.ac.uk/staff/d.barber/brml/) Cambridge University Press, 2012.
* Murphy K.P. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.
* Bishop C.M. [Pattern Recognition and Machine Learning.](http://research.microsoft.com/en-us/um/people/cmbishop/prml/) Springer, 2006. 
* Mackay D.J.C. [Information Theory, Inference, and Learning Algorithms.](http://www.inference.phy.cam.ac.uk/mackay/itila/book.html) Cambridge University Press, 2003. 
* Tipping M. [Sparse Bayesian Learning.](http://www.jmlr.org/papers/volume1/tipping01a/tipping01a.pdf) Journal of Machine Learning Research, 1, 2001, pp. 211-244. 
* Шумский С.А. [Байесова регуляризация обучения.](http://www.niisi.ru/iont/ni/Library/School-2002/Shumsky-2002.pdf) В сб. Лекции по нейроинформатике, часть 2, 2002.

Простые и удобные [заметки](http://cs.nyu.edu/~roweis/notes.html) по матричным вычислениям и свойствам гауссовских распределений.

[Памятка](http://statistics.zone/) по теории вероятностей.
