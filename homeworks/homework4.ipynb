{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа по Relevance Vector Regression\n",
    "В рамках этой лабораторной работы необходимо:\n",
    "- Имплементировать Relevance Vector Regression\n",
    "- Применить на синетическом датасете (восстановление полинома), сравнить с Lasso из sklearn и гребневой регрессией\n",
    "- Применить на данных sinc с RBF признаками, визуализировать \"релевантные вектора\", сравнить с Support Vector Regression и Lasso\n",
    "- Сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(123)\n",
    "\n",
    "def l2_error(X, t, w):\n",
    "    return np.sum((X.dot(w.ravel()) - t) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Имплементация Relevance Vector Regression\n",
    "\n",
    "Здесь необходимо реализовать три функции:\n",
    "\n",
    "1. `get_w_sigma(X, t, alpha, beta)`, которая принимает датасет (X, t) и гиперпараметры RVR (alpha, beta) и возвращает параметры апостериорного распределения mu, sigma\n",
    "2. `update_alpha_beta(X, t, alpha, beta)`, которая принимает датасет (X, t) и гиперпараметры RVR (alpha, beta) и делает один шаг итерационной процедуры для обновления гиперпараметров (было на лекции)\n",
    "3. `fit_rvr(X, t, max_iters)`, которая принимает датасет (X, t) и максимальное количество итераций и возвращает обученные гиперпараметры и параметры апостериорного распределения на веса модели\n",
    "\n",
    "На что стоит обратить внимание:\n",
    "\n",
    "1. Результаты дорогостоящих операций типа перемножения одних и тех же матриц можно кешировать и переиспользовать\n",
    "2. $\\alpha$-ы для нерелевантных объектов должны принять значение `np.inf`, а соответствующие веса и их дисперсии должны иметь значение 0\n",
    "3. Бесконечности и нули из предыдущего пункта должны обрабатываться корректно, без NaN-ов и warning-ов\n",
    "4. Матрицу с бесконечными элементами на диагонали можно обращать более эффективно (достаточно обратить подматрицу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w_sigma(X, t, alpha, beta):\n",
    "    \"\"\"Calculate the mean and the covariance matrix\n",
    "       of the posterior distribution\"\"\"\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    return w, sigma\n",
    "\n",
    "\n",
    "def update_alpha_beta(X, t, alpha, beta):\n",
    "    \"\"\"Update the hyperperemeters to increase evidence\"\"\"\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    return alpha_new, beta_new\n",
    "\n",
    "\n",
    "def fit_rvr(X, t, max_iter=10000):\n",
    "    \"\"\"Train the Relevance Vector Regression model\"\"\"\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "    return w, sigma, alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Восстановление полинома\n",
    "\n",
    "Здесь решается модельная задача: зашумленным полиномом третьей степени сгенерированы данные для задачи регрессии. Нужно на этих данных обучить многочлен степени, не превышающей 20. Предлагается сравнить три модели: гребневую регрессию, L1-регрессию (Lasso) и RVR, и сравнить ошибку на тестовой выборке и качество отобранных признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data generation\n",
    "\n",
    "def gen_batch(n, w, beta):\n",
    "    d = len(w)\n",
    "    X = np.random.uniform(-10, 10, (n, 1))\n",
    "    X = np.sort(X, axis=0)\n",
    "    X = np.hstack([X ** i for i in range(d)])\n",
    "    t = X.dot(w) + np.random.normal(size=n) / beta ** 0.5\n",
    "    return X, t\n",
    "\n",
    "n = 200\n",
    "d = 21\n",
    "w_true = np.zeros(d)\n",
    "w_true[1] = 100\n",
    "w_true[3] = -1\n",
    "beta_true = 1e-4\n",
    "\n",
    "X_train, t_train = gen_batch(n, w_true, beta_true)\n",
    "X_test, t_test = gen_batch(n, w_true, beta_true)\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X_train[:, 1], t_train, s=3, label='Train data', alpha=0.3)\n",
    "plt.scatter(X_test[:, 1], t_test, s=3, label='Test data', alpha=0.3)\n",
    "plt.plot(X_train[:, 1], X_train.dot(w_true), label='Ground truth')\n",
    "\n",
    "plt.axes().set_xlabel('x')\n",
    "plt.axes().set_ylabel('y')\n",
    "plt.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relevance Vector Regression\n",
    "w_rvr, sigma_rvr, alpha_rvr, beta_rvr = fit_rvr(X_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge Regression with Cross-Validation\n",
    "from sklearn.linear_model import RidgeCV\n",
    "ridge = RidgeCV(cv=20, alphas=10.**np.arange(-6, 3, 1),\n",
    "                fit_intercept=False).fit(X_train, t_train)\n",
    "w_ridge = ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lasso Regression with Cross-Validation\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "lasso = LassoCV(cv=20, alphas=10.**np.arange(-6, 3, 1),\n",
    "                fit_intercept=False).fit(X_train, t_train)\n",
    "w_lasso = lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Comparison\n",
    "print('Relevance Vector Regression')\n",
    "print('Features remaining:', np.sum(alpha_rvr < 1e8), '/', d)\n",
    "print('Train error:', l2_error(X_train, t_train, w_rvr) / n)\n",
    "print('Test error: ', l2_error(X_test, t_test, w_rvr) / n)\n",
    "print('-'*50)\n",
    "print('Ridge Regression')\n",
    "print('Features remaining: NA (no sparsity)')\n",
    "print('Train error:', l2_error(X_train, t_train, w_ridge) / n)\n",
    "print('Test error: ', l2_error(X_test, t_test, w_ridge) / n)\n",
    "print('-'*50)\n",
    "print('Lasso Regression')\n",
    "print('Features remaining:', np.sum(np.abs(w_lasso) > 1e-20), '/', d)\n",
    "print('Train error:', l2_error(X_train, t_train, w_lasso) / n)\n",
    "print('Test error: ', l2_error(X_test, t_test, w_lasso) / n)\n",
    "\n",
    "plt.scatter(X_train[:, 1], t_train, s=3, label='Train data', alpha=0.3)\n",
    "plt.scatter(X_test[:, 1], t_test, s=3, label='Test data', alpha=0.3)\n",
    "plt.plot(X_train[:, 1], X_train.dot(w_true), label='Ground truth')\n",
    "plt.plot(X_train[:, 1], X_train.dot(w_rvr), label='RVR')\n",
    "plt.plot(X_train[:, 1], X_train.dot(w_ridge), label='Ridge')\n",
    "plt.plot(X_train[:, 1], X_train.dot(w_lasso), label='Lasso')\n",
    "\n",
    "plt.axes().set_xlabel('x')\n",
    "plt.axes().set_ylabel('y')\n",
    "plt.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия с RBF-признаками\n",
    "\n",
    "Здесь решается другая модельная задача: необходимо восстановить зашумленную функцию `sinc(x)`. Предлагается применить kernel trick с RBF-ядром (можно использовать функцию `sklearn.metrics.pairwise.rbf_kernel`), обучить три модели: SVM-регрессию (SVR), L1-регрессию (Lasso) и RVR, и сравнить ошибку на тестовой выборке и качество отобранных опорных / релевантных объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def gen_batch(n, beta):\n",
    "    points = np.random.uniform(-5, 5, n)\n",
    "    points = np.sort(points)\n",
    "    t = np.sinc(points) + np.random.normal(size=n) / beta ** 0.5\n",
    "    return points, t\n",
    "\n",
    "n = 500\n",
    "d = n + 1\n",
    "beta_true = 100\n",
    "\n",
    "points_train, t_train = gen_batch(n, beta_true)\n",
    "points_test, t_test = gen_batch(n, beta_true)\n",
    "\n",
    "# RBF-transform\n",
    "X_train = # YOUR CODE GOES HERE\n",
    "X_test = # YOUR CODE GOES HERE\n",
    "\n",
    "# Constant feature\n",
    "X_train = np.hstack((np.ones((n, 1)), X_train))\n",
    "X_test = np.hstack((np.ones((n, 1)), X_test))\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(points_train, t_train, s=3, label='Train data', alpha=0.3)\n",
    "plt.scatter(points_test, t_test, s=3, label='Test data', alpha=0.3)\n",
    "plt.plot(points_train, np.sinc(points_train), label='Ground truth')\n",
    "\n",
    "plt.axes().set_xlabel('x')\n",
    "plt.axes().set_ylabel('y')\n",
    "plt.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relevance Vector Regression\n",
    "w_rvr, sigma_rvr, alpha_rvr, beta_rvr = fit_rvr(X_train, t_train, max_iter=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lasso Regression with Cross-Validation\n",
    "from sklearn.linear_model import LassoCV\n",
    "lasso = LassoCV(cv=10, alphas=10.**np.arange(-6, 3, 1),\n",
    "                fit_intercept=False).fit(X_train, t_train)\n",
    "w_lasso = lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Regression\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(gamma=1, tol=1e-6, C=1).fit(points_train.reshape(-1, 1), t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Comparison\n",
    "print('Relevance Vector Regression')\n",
    "print('Objects remaining:', np.sum(alpha_rvr[1:] < 1e8), '/', n)\n",
    "print('Train error:', l2_error(X_train, t_train, w_rvr) / n)\n",
    "print('Test error: ', l2_error(X_test, t_test, w_rvr) / n)\n",
    "print('-'*50)\n",
    "print('Lasso Regression')\n",
    "print('Objects remaining:', np.sum(np.abs(w_lasso[1:]) > 1e-20), '/', n)\n",
    "print('Train error:', l2_error(X_train, t_train, w_lasso) / n)\n",
    "print('Test error: ', l2_error(X_test, t_test, w_lasso) / n)\n",
    "print('-'*50)\n",
    "print('Support Vector Regression')\n",
    "print('Objects remaining:', len(svr.support_), '/', n)\n",
    "print('Train error:', np.sum((svr.predict(points_train.reshape(-1, 1)) - t_train) ** 2) / n)\n",
    "print('Test error: ', np.sum((svr.predict(points_test.reshape(-1, 1)) - t_test) ** 2) / n)\n",
    "\n",
    "plt.scatter(points_train, t_train, s=3, label='Train data', alpha=0.3)\n",
    "plt.scatter(points_test, t_test, s=3, label='Test data', alpha=0.3)\n",
    "plt.plot(points_train, np.sinc(points_train), label='Ground truth')\n",
    "plt.plot(points_train, X_train.dot(w_rvr), label='RVR')\n",
    "plt.plot(points_train, X_train.dot(w_lasso), label='Lasso')\n",
    "plt.plot(points_train, svr.predict(points_train.reshape(-1, 1)), label='SVR')\n",
    "\n",
    "plt.axes().set_xlabel('x')\n",
    "plt.axes().set_ylabel('y')\n",
    "plt.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Визуализация релевантных объектов для RVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant = alpha_rvr[1:] < 1e8\n",
    "plt.scatter(points_train, t_train, s=3, label='Train data', alpha=0.3)\n",
    "plt.scatter(points_train[relevant], t_train[relevant], c='tab:blue', s=30, label='Relevant objects')\n",
    "plt.scatter(points_test, t_test, s=3, label='Test data', alpha=0.3)\n",
    "plt.plot(points_train, np.sinc(points_train), label='Ground truth')\n",
    "plt.plot(points_train, X_train.dot(w_rvr), label='RVR')\n",
    "\n",
    "plt.axes().set_xlabel('x')\n",
    "plt.axes().set_ylabel('y')\n",
    "plt.legend(ncol=3, loc=9, bbox_to_anchor=(0.5, 1.25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "В этом поле опишите свои наблюдения и сформулируйте свои выводы"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
