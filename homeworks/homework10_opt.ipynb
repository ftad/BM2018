{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание по байесовской оптимизации\n",
    "\n",
    "Задание необязательное (баллы не учитываются в нормировке)\n",
    "\n",
    "Сумма баллов: 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение регрессии с помощью Гауссовского процесса (5 баллов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация выборки\n",
    "\n",
    "Ответ на очередном объекте $x_i$ генерируется следующим образом:\n",
    "\n",
    "$$t_i = f(x_i)+\\varepsilon_i, \\; \\varepsilon_i \\sim \\mathcal{N}(0,\\sqrt{2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -8*np.sin(x)\n",
    "\n",
    "support = np.arange(-1.0, 11.0, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(0,10.0,0.5)\n",
    "X_train = X_train.reshape([len(X_train), -1])\n",
    "Y_train = f(X_train)\n",
    "T_train = Y_train+2*np.random.randn(*X_train.shape)\n",
    "\n",
    "X_test = support\n",
    "X_test = X_test.reshape([len(X_test), -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(16,8)\n",
    "plt.scatter(X_train.flatten(), T_train.flatten(), label='data', c='r', s=40)\n",
    "plt.plot(support, f(support), '--r', label='true function')\n",
    "plt.legend(fontsize=20, frameon=True, shadow=True)\n",
    "plt.xlim(np.min(support), np.max(support))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предсказание\n",
    "\n",
    "В тестовой точке $x$ мы можем предсказать два параметра – мат. ожидание $\\mu(x)$ и стандартное отклонение $\\sigma(x)$.\n",
    "Для заданной ковариационной функции $k(x,y)$ и для заданной дисперсии случайного шума $\\beta^{-1}$, $\\mu(x), \\sigma(x)$ вычисляются по следующим формулам:\n",
    "\n",
    "$$\\mu(x) = \\mathbf{k}^TC_N^{-1}\\mathbf{t}, \\;\\;\\;\\; \\sigma(x) = k(x,x)+\\beta^{-1}-\\mathbf{k}^TC_N^{-1}\\mathbf{k},$$\n",
    "\n",
    "где $\\mathbf{k} = k(x_i, x)$ – вектор, состоящий из элементов $k(x_i, x), \\; i=1,\\ldots,N$,  \n",
    "$\\mathbf{t} = (t_1,\\ldots,t_N)$ – вектор ответов на обучающей выборке,  \n",
    "$С_N = \\mathbf{K} + \\beta^{-1}E$, где $\\mathbf{K}_{ij} = k(x_i,x_j)$ – ковариационная матрица.\n",
    "\n",
    "Примеры ковариационных функций:\n",
    "- $K(x,y) = C$ – константная,\n",
    "- $K(x,y) = \\langle x,y \\rangle$ – линейная,\n",
    "- $K(x,y) = \\exp(-\\sum_{j=1}^d\\theta_j(x_j-y_j)^2)$  – экспоненциальная,\n",
    "- $K(x,y) = \\exp(-\\sum_{j=1}^d\\theta_j|x_j-y_j|)$ – процесс Орнштейна-Уленбека."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Реализуте функции вычисления ковариационных матриц между двумя произвольными выборками объектов, а затем функцию, вычисляющую предсказание в новой точке. Циклы использовать запрещено."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_k(X, Y, theta):\n",
    "    # your code here\n",
    "    \n",
    "def rbf_k(X, Y, theta):\n",
    "    # your code here\n",
    "\n",
    "def ou_k(X, Y, theta):\n",
    "    # your code here\n",
    "\n",
    "def predict(X_train, T_train, X_test, theta, beta, kernel):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_result():\n",
    "    figsize(16,8)\n",
    "    plt.scatter(X_train.flatten(), T_train.flatten(), label='data', c='r', s=40)\n",
    "    plt.plot(support, f(support), '--r', label='true function')\n",
    "    plt.plot(X_test, mu_test, c='b', label='mu test')\n",
    "    plt.fill_between(X_test.flatten(), mu_test-sigma_test, mu_test+sigma_test, color='b', label='confidence', alpha=0.3)\n",
    "    plt.legend(fontsize=20, frameon=True, shadow=True)\n",
    "    plt.xlim(np.min(support), np.max(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Экспоненциальная ковариационная функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.array([1.0])\n",
    "beta = 100.0\n",
    "kernel = rbf_k\n",
    "mu_test, sigma_test = predict(X_train, T_train, X_test, theta, beta, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как ведёт себя регрессия для разных значений $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "theta = np.array([1.0])\n",
    "beta = 100.0\n",
    "kernel = rbf_k\n",
    "mu_test, sigma_test = predict(X_train, T_train, X_test, theta, beta, kernel)\n",
    "plot_result()\n",
    "plt.subplot(122)\n",
    "theta = np.array([1.0])\n",
    "beta = 1.0\n",
    "kernel = rbf_k\n",
    "mu_test, sigma_test = predict(X_train, T_train, X_test, theta, beta, kernel)\n",
    "plot_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишите что вы пронаблюдали и как можно интерпретировать параметр $\\beta$.  \n",
    "(_пункт не оценивается_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная ковариационная функция"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.array([1.0])\n",
    "beta = 100.0\n",
    "kernel = linear_k\n",
    "mu_test, sigma_test = predict(X_train, T_train, X_test, theta, beta, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ковариационная функция Орнштейна-Уленбека"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0])\n",
    "beta = 100.0\n",
    "kernel = ou_k\n",
    "mu_test, sigma_test = predict(X_train, T_train, X_test, theta, beta, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике часто прибегают к смеси ковариационных функций, чтобы добиться определённых свойств регрессии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0])\n",
    "beta = 10.0\n",
    "kernel = lambda X,Y,theta: ou_k(X,Y,theta) + rbf_k(X,Y,theta)\n",
    "mu_test, sigma_test = predict(X_train, T_train, X_test, theta, beta, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров (3 балла)\n",
    "\n",
    "Подбор параметров $\\theta$ для произвольного ядра можно сделать максимизируя правдоподобие на обучающей выборке:\n",
    "\n",
    "$$\\log p(\\mathbf{t}|\\theta) = -\\frac{1}{2}\\log|C_N| - \\frac{1}{2}\\mathbf{t}^TC_N^{-1}\\mathbf{t} - \\frac{N}{2}\\log(2\\pi) \\to \\max_{\\theta}.$$\n",
    "\n",
    "Максимизировать заданный функционал мы будем с помощью градиентного подъёма, для чего нам понадобится формула для градиента по $\\theta$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j}\\log p(\\mathbf{t}|\\theta) = -\\frac{1}{2}\\text{Tr}\\bigg(C_N^{-1}\\frac{\\partial C_N}{\\partial \\theta_j}\\bigg) + \\frac{1}{2}\\mathbf{t}^TC_N^{-1}\\frac{\\partial C_N}{\\partial \\theta_j}C_N^{-1}\\mathbf{t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Реализуйте функцию для расчёта градиента логарифма правдоподобия по логарифму $\\theta$, а также функцию для расчёта правдоподобия. Ядро – RBF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rbf_grad_log_theta(X, Y, theta):\n",
    "    # your code here\n",
    "\n",
    "def get_likelihood(X, Y, theta):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "num_iterations = 100\n",
    "beta = 100.0\n",
    "log_theta = np.log(np.array([10.0]))\n",
    "likelihood = []\n",
    "theta_history = []\n",
    "for _ in range(num_iterations):\n",
    "    theta_history.append(np.exp(log_theta))\n",
    "    log_theta = log_theta + lr*get_rbf_grad_log_theta(X_train, T_train, np.exp(log_theta))\n",
    "    likelihood.append(get_likelihood(X_train, T_train, np.exp(log_theta)))\n",
    "likelihood = np.array(likelihood).flatten()\n",
    "theta_history = np.array(theta_history).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(likelihood, label='likelihood', c='b')\n",
    "ax1.set_xlabel('iteration', fontsize=20)\n",
    "ax1.set_ylabel('likelihood', fontsize=20)\n",
    "ax1.grid()\n",
    "\n",
    "ax2.plot(theta_history, label='theta', c='g')\n",
    "ax2.set_ylabel('theta', fontsize=20)\n",
    "\n",
    "h1, l1 = ax1.get_legend_handles_labels()\n",
    "h2, l2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(h1+h2, l1+l2, fontsize=20, loc='center right', frameon=True, shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 100.0\n",
    "kernel = rbf_k\n",
    "mu_test, sigma_test = predict(X_train, T_train, X_test, np.exp(log_theta), beta, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравните этот график с графиком для $\\theta=1$. Что произошло с мат. ожиданием? Как изменился параметр $\\theta$?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация с помощью Гауссовского процесса (7 баллов)\n",
    "В этом разделе понадобятся функции из предыдущего раздела!\n",
    "\n",
    "\n",
    "С помощью Гауссовского процесса можно решать задачи black-box оптимизации. То есть, оптимизировать функцию, вид которой нам не известен и нет никакой дополнительной информации, кроме её значений в некоторых точках. Такой метод оптимизации может использоваться для поиска экстремумов некоторой функции, которую очень дорого вычислять, а градиенты этой функции мы и вовсе не можем рассчитать.\n",
    "\n",
    "Концепция такого алгоритма оптимизации следующая:\n",
    "1. Настраиваем параметры Гауссовского процесса для имеющегося набора точек.\n",
    "2. В каждой точке пространства $x$ мы можем посчитать величину expected improvement:\n",
    "$$\\text{EI}(x) = \\mathbb{E}_{y(x)}\\max(0,t_{\\min} - y(x)),$$\n",
    "где $t_{\\min}$ – минимальное значение целевой переменной на уже имеющейся выборке.  \n",
    "На практике же, мы не можем посчитать expected improvement в каждой точке пространства, но мы можем оптимизировать эту функцию относительно $x$ методами первого или второго порядка и найти локальный максимум этой функции.\n",
    "3. В качестве новой точки выбираем $x^* = \\text{argmax}\\; \\text{EI}(x)$ и рассчитываем значение $t^*$ в этой точке. После чего добавляем эту точку к нашим данным и возвращаемся к шагу 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -1/np.sqrt(2*np.pi*0.25)*np.exp(-0.5/0.09*(x-4)**2)-1/np.sqrt(2*np.pi*1.0)*np.exp(-0.5/1.0*(x-8)**2)\n",
    "\n",
    "support = np.arange(0.95,11.1,1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(2.0,10.0,3.0)\n",
    "X_train = X_train + (2.*np.random.rand(len(X_train))-1.)\n",
    "X_train = X_train.reshape([len(X_train), -1])\n",
    "Y_train = f(X_train)\n",
    "T_train = Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize(16,8)\n",
    "plt.scatter(X_train.flatten(), T_train.flatten(), label='data', c='r', s=40)\n",
    "plt.plot(support, f(support), '--r', label='true function')\n",
    "plt.legend(fontsize=20, frameon=True, shadow=True)\n",
    "plt.xlim(np.min(support), np.max(support))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. ** Выведите формулу для expected improvement.\n",
    "\n",
    "_Приведите свои рассуждения здесь_\n",
    "\n",
    "**4. ** Реализуйте функцию для расчёта expected improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def get_expected_improvement(X_train, T_train, X_test, theta, beta, kernel):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = support\n",
    "X_test = X_test.reshape([len(X_test), -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_improvement(X_train, T_train, expected_improvement, predictions):\n",
    "    fig, ax1 = plt.subplots(1,1)\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.scatter(X_train.flatten(), T_train.flatten(), label='data', c='r', s=40)\n",
    "    ax1.plot(support, predictions, label='mean')\n",
    "    ax1.plot(support, f(support), '--r', label='true function')\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.plot(support, expected_improvement, label='Expected improvement', c='g')\n",
    "    ax2.scatter(support[np.argmax(expected_improvement)], np.max(expected_improvement), label='max', c='g', s=40)\n",
    "    ax2.set_ylabel('Expected improvement', fontsize=20)\n",
    "\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1+h2, l1+l2, fontsize=10, frameon=True, shadow=True)\n",
    "    plt.xlim(np.min(support), np.max(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите процесс оптимизации. Сошёлся ли он в точку минимума? Попробуйте разные значения параметров.  \n",
    "(_пункт не оценивается_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log_theta = np.log(np.array([10.0]))\n",
    "beta = 100.0\n",
    "num_tuning_steps = 200\n",
    "num_new_points = 20\n",
    "lr = 1e-3\n",
    "for _ in range(num_new_points):\n",
    "    plt.clf()\n",
    "    # model tuning  \n",
    "    for _ in range(num_tuning_steps):\n",
    "        grad = get_rbf_grad_log_theta(X_train, T_train, np.exp(log_theta))\n",
    "        log_theta = log_theta + lr*grad\n",
    "    # calculating expected improvement for all points\n",
    "    expected_improvement = get_expected_improvement(X_train, T_train, X_test, np.exp(log_theta), beta, kernel)\n",
    "    mu, sigma = predict(X_train, T_train, X_test, np.exp(log_theta), beta, kernel)\n",
    "    plot_expected_improvement(X_train, T_train, expected_improvement, mu)\n",
    "    # adding new point\n",
    "    X_train = np.vstack([X_train, np.array(support[np.argmax(expected_improvement)])])\n",
    "    T_train = f(X_train)\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "_ = plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
